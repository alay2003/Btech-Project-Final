{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T07:46:36.868391Z",
     "iopub.status.busy": "2024-11-07T07:46:36.868083Z",
     "iopub.status.idle": "2024-11-07T07:46:50.157234Z",
     "shell.execute_reply": "2024-11-07T07:46:50.156088Z",
     "shell.execute_reply.started": "2024-11-07T07:46:36.868356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\alayp\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\alayp\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement skcikit-learn (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for skcikit-learn\n"
     ]
    }
   ],
   "source": [
    "pip install skcikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T07:46:50.160319Z",
     "iopub.status.busy": "2024-11-07T07:46:50.159632Z",
     "iopub.status.idle": "2024-11-07T07:47:02.426944Z",
     "shell.execute_reply": "2024-11-07T07:47:02.425656Z",
     "shell.execute_reply.started": "2024-11-07T07:46:50.160268Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.10.10-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.1-cp312-cp312-win_amd64.whl.metadata (66 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Using cached propcache-0.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.0 MB 25.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.2/10.0 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.0 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.6/10.0 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.0 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached aiohttp-3.10.10-cp312-cp312-win_amd64.whl (379 kB)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.0.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/25.1 MB 9.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 7.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.7/25.1 MB 7.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.0/25.1 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.6/25.1 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.9/25.1 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/25.1 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.5/25.1 MB 6.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.8/25.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.2/25.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.5/25.1 MB 6.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.0/25.1 MB 6.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.4/25.1 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.7/25.1 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/25.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.3/25.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/25.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 6.6 MB/s eta 0:00:00\n",
      "Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 8.5 MB/s eta 0:00:00\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.17.1-cp312-cp312-win_amd64.whl (89 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Installing collected packages: xxhash, safetensors, pyarrow, propcache, multidict, fsspec, frozenlist, filelock, dill, attrs, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 datasets-3.1.0 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.26.2 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-18.0.0 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.2 xxhash-3.5.0 yarl-1.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T15:14:42.547420Z",
     "iopub.status.busy": "2024-11-07T15:14:42.546533Z",
     "iopub.status.idle": "2024-11-07T15:14:42.607482Z",
     "shell.execute_reply": "2024-11-07T15:14:42.606598Z",
     "shell.execute_reply.started": "2024-11-07T15:14:42.547370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/alay-data/bart_model/config.json\n",
      "/kaggle/input/alay-data/bart_model/model.safetensors\n",
      "/kaggle/input/alay-data/bart_model/generation_config.json\n",
      "/kaggle/input/alay-data/bart_tokenizer/merges.txt\n",
      "/kaggle/input/alay-data/bart_tokenizer/vocab.json\n",
      "/kaggle/input/alay-data/bart_tokenizer/tokenizer_config.json\n",
      "/kaggle/input/alay-data/bart_tokenizer/special_tokens_map.json\n",
      "/kaggle/input/conversational-question-answering-dataset-coqa/coqa-dev-v1.0.json\n",
      "/kaggle/input/conversational-question-answering-dataset-coqa/coqa-train-v1.0.json\n",
      "/kaggle/input/btech-data/merges.txt\n",
      "/kaggle/input/btech-data/vocab.json\n",
      "/kaggle/input/btech-data/tokenizer_config.json\n",
      "/kaggle/input/btech-data/special_tokens_map.json\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-test.ref\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-test.tsv\n",
      "/kaggle/input/wikiqa-corpus/WikiQA.tsv\n",
      "/kaggle/input/wikiqa-corpus/LICENSE.pdf\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-train.tsv\n",
      "/kaggle/input/wikiqa-corpus/Guidelines_Phase1.pdf\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-dev.txt\n",
      "/kaggle/input/wikiqa-corpus/eval.py\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-dev.tsv\n",
      "/kaggle/input/wikiqa-corpus/README.txt\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-dev-filtered.ref\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-test-filtered.ref\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-dev.ref\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-test.txt\n",
      "/kaggle/input/wikiqa-corpus/Guidelines_Phase2.pdf\n",
      "/kaggle/input/wikiqa-corpus/WikiQASent.pos.ans.tsv\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-train.ref\n",
      "/kaggle/input/wikiqa-corpus/WikiQA-train.txt\n",
      "/kaggle/input/wikiqa-corpus/emnlp-table/WikiQA.CNN.dev.rank\n",
      "/kaggle/input/wikiqa-corpus/emnlp-table/WikiQA.CNN-Cnt.dev.rank\n",
      "/kaggle/input/wikiqa-corpus/emnlp-table/WikiQA.CNN.test.rank\n",
      "/kaggle/input/wikiqa-corpus/emnlp-table/WikiQA.CNN-Cnt.test.rank\n",
      "/kaggle/input/open-trivia-database-quiz-questions-all-categories/quiz_questions_column_descriptors.txt\n",
      "/kaggle/input/open-trivia-database-quiz-questions-all-categories/quiz_questions.csv\n",
      "/kaggle/input/squad-20/train-v2.0.json\n",
      "/kaggle/input/squad-20/dev-v2.0.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T07:47:14.973761Z",
     "iopub.status.busy": "2024-11-07T07:47:14.973376Z",
     "iopub.status.idle": "2024-11-07T07:47:27.388086Z",
     "shell.execute_reply": "2024-11-07T07:47:27.386938Z",
     "shell.execute_reply.started": "2024-11-07T07:47:14.973722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\alayp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T15:25:10.305118Z",
     "iopub.status.busy": "2024-11-07T15:25:10.303966Z",
     "iopub.status.idle": "2024-11-07T15:25:12.544602Z",
     "shell.execute_reply": "2024-11-07T15:25:12.543637Z",
     "shell.execute_reply.started": "2024-11-07T15:25:10.305063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQuAD Sample Data:\n",
      "                                            question               answer\n",
      "0           When did Beyonce start becoming popular?    in the late 1990s\n",
      "1  What areas did Beyonce compete in when she was...  singing and dancing\n",
      "2  When did Beyonce leave Destiny's Child and bec...                 2003\n",
      "3      In what city and state did Beyonce  grow up?        Houston, Texas\n",
      "4         In which decade did Beyonce become famous?           late 1990s\n",
      "\n",
      "COQA Sample Data:\n",
      "                            question                               answer\n",
      "0  When was the Vat formally opened?  It was formally established in 1475\n",
      "1           what is the library for?                             research\n",
      "2                 for what subjects?                     history, and law\n",
      "3                               and?     philosophy, science and theology\n",
      "4          what was started in 2014?                           a  project\n",
      "\n",
      "WikiQA Sample Data:\n",
      "                        question  \\\n",
      "0  how are glacier caves formed?   \n",
      "1  how are glacier caves formed?   \n",
      "2  how are glacier caves formed?   \n",
      "3  how are glacier caves formed?   \n",
      "4  how are glacier caves formed?   \n",
      "\n",
      "                                              answer  \n",
      "0  A partly submerged glacier cave on Perito More...  \n",
      "1          The ice facade is approximately 60 m high  \n",
      "2          Ice formations in the Titlis glacier cave  \n",
      "3  A glacier cave is a cave formed within the ice...  \n",
      "4  Glacier caves are often called ice caves , but...  \n",
      "\n",
      "Open Trivia Sample Data:\n",
      "                                            question  answer\n",
      "0  The Great Wall of China is visible from the moon.   False\n",
      "1  Jingle Bells was originally meant for Thanksgi...    True\n",
      "2  Nutella is produced by the German company Ferr...   False\n",
      "3  The bikini is named after the &quot;Bikini Ato...    True\n",
      "4          French is an official language in Canada.    True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Helper function to load and preprocess datasets\n",
    "def load_and_preprocess(path, dataset_type='json', sample_size=1000, columns=None):\n",
    "    if dataset_type == 'json':\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return process_json_data(data, sample_size)\n",
    "    elif dataset_type == 'csv':\n",
    "        return process_csv_data(path, sample_size, columns)\n",
    "    elif dataset_type == 'tsv':\n",
    "        return process_tsv_data(path, sample_size, columns)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n",
    "\n",
    "# Function to process JSON datasets (for SQuAD, COQA)\n",
    "def process_json_data(data, sample_size=1000):\n",
    "    processed_data = []\n",
    "    if 'data' in data:  # SQuAD or COQA structure\n",
    "        for article in data['data']:\n",
    "            for paragraph in article.get('paragraphs', []):\n",
    "                for qa in paragraph.get('qas', []):\n",
    "                    question = qa.get('question')\n",
    "                    answer = qa['answers'][0].get('text') if qa['answers'] else None\n",
    "                    processed_data.append({'question': question, 'answer': answer})\n",
    "                    if len(processed_data) >= sample_size:\n",
    "                        return pd.DataFrame(processed_data)\n",
    "    elif 'annotations' in data:  # Other QA datasets\n",
    "        for item in data.get('annotations', []):\n",
    "            question = item['question_text']\n",
    "            answer = item['short_answers'][0].get('text') if item['short_answers'] else None\n",
    "            processed_data.append({'question': question, 'answer': answer})\n",
    "            if len(processed_data) >= sample_size:\n",
    "                return pd.DataFrame(processed_data)\n",
    "\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Function to process CSV data (e.g., Open Trivia)\n",
    "def process_csv_data(path, sample_size=1000, columns=None):\n",
    "    df = pd.read_csv(path, usecols=columns, nrows=sample_size)\n",
    "    df = df.rename(columns={'correct_answer': 'answer'})\n",
    "    return df[['question', 'answer']]\n",
    "\n",
    "# Function to process large CSV files in chunks\n",
    "def process_large_csv(path, sample_size=1000, chunksize=10000):\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(path, usecols=['question', 'correct_answer'], chunksize=chunksize):\n",
    "        chunk = chunk.rename(columns={'correct_answer': 'answer'})\n",
    "        chunk_list.append(chunk[['question', 'answer']])\n",
    "        if len(chunk_list) * chunksize >= sample_size:\n",
    "            break\n",
    "    return pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Function to process TSV data for WikiQA\n",
    "def process_tsv_data(path, sample_size=1000, columns=['Question', 'Sentence', 'Label']):\n",
    "    df = pd.read_csv(path, sep='\\t', usecols=columns, nrows=sample_size)\n",
    "    df = df.rename(columns={'Question': 'question', 'Sentence': 'answer'})\n",
    "    return df[['question', 'answer']]\n",
    "\n",
    "# Load and preprocess SQuAD dataset\n",
    "def preprocess_squad(path, sample_size=1000):\n",
    "    with open(path, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "    return process_json_data(squad_data, sample_size=sample_size)\n",
    "\n",
    "# Load and preprocess COQA dataset\n",
    "def preprocess_coqa(path, sample_size=1000):\n",
    "    with open(path, 'r') as f:\n",
    "        coqa_data = json.load(f)\n",
    "    \n",
    "    coqa_data_processed = []\n",
    "    for dialog in coqa_data['data']:\n",
    "        for q, a in zip(dialog['questions'], dialog['answers']):\n",
    "            question = q['input_text']\n",
    "            answer = a['input_text']\n",
    "            coqa_data_processed.append({'question': question, 'answer': answer})\n",
    "            if len(coqa_data_processed) >= sample_size:\n",
    "                break\n",
    "        if len(coqa_data_processed) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(coqa_data_processed)\n",
    "\n",
    "# Load and preprocess Open Trivia dataset\n",
    "def preprocess_open_trivia(path, sample_size=1000):\n",
    "    return process_large_csv(path, sample_size=sample_size)\n",
    "\n",
    "# Load and preprocess WikiQA dataset from TSV format\n",
    "def preprocess_wikiqa(path, sample_size=1000):\n",
    "    return process_tsv_data(path, sample_size=sample_size)\n",
    "\n",
    "# Paths to datasets\n",
    "squad_path = '/kaggle/input/squad-20/train-v2.0.json'  # SQuAD dataset\n",
    "coqa_path = '/kaggle/input/conversational-question-answering-dataset-coqa/coqa-train-v1.0.json'  # COQA dataset\n",
    "wikiqa_path = '/kaggle/input/wikiqa-corpus/WikiQA-train.tsv'  # WikiQA dataset\n",
    "trivia_path = '/kaggle/input/open-trivia-database-quiz-questions-all-categories/quiz_questions.csv'  # Open Trivia dataset\n",
    "\n",
    "# Process and display sample data from each dataset\n",
    "squad_df = preprocess_squad(squad_path)\n",
    "print(\"\\nSQuAD Sample Data:\")\n",
    "print(squad_df.head())\n",
    "\n",
    "coqa_df = preprocess_coqa(coqa_path)\n",
    "print(\"\\nCOQA Sample Data:\")\n",
    "print(coqa_df.head())\n",
    "\n",
    "wikiqa_df = preprocess_wikiqa(wikiqa_path)\n",
    "print(\"\\nWikiQA Sample Data:\")\n",
    "print(wikiqa_df.head())\n",
    "\n",
    "trivia_df = preprocess_open_trivia(trivia_path)\n",
    "print(\"\\nOpen Trivia Sample Data:\")\n",
    "print(trivia_df.head())\n",
    "\n",
    "# Free up memory\n",
    "del squad_df, coqa_df, wikiqa_df, trivia_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T15:28:03.299125Z",
     "iopub.status.busy": "2024-11-07T15:28:03.298388Z",
     "iopub.status.idle": "2024-11-07T15:28:05.090926Z",
     "shell.execute_reply": "2024-11-07T15:28:05.090067Z",
     "shell.execute_reply.started": "2024-11-07T15:28:03.299086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Dataset Sample:\n",
      "                                            question               answer\n",
      "0           When did Beyonce start becoming popular?    in the late 1990s\n",
      "1  What areas did Beyonce compete in when she was...  singing and dancing\n",
      "2  When did Beyonce leave Destiny's Child and bec...                 2003\n",
      "3      In what city and state did Beyonce  grow up?        Houston, Texas\n",
      "4         In which decade did Beyonce become famous?           late 1990s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Helper function to load and preprocess datasets\n",
    "def load_and_preprocess(path, dataset_type='json', sample_size=1000, columns=None):\n",
    "    if dataset_type == 'json':\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return process_json_data(data, sample_size)\n",
    "    elif dataset_type == 'csv':\n",
    "        return process_csv_data(path, sample_size, columns)\n",
    "    elif dataset_type == 'tsv':\n",
    "        return process_tsv_data(path, sample_size, columns)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {dataset_type}\")\n",
    "\n",
    "# Function to process JSON datasets (for SQuAD, COQA)\n",
    "def process_json_data(data, sample_size=1000):\n",
    "    processed_data = []\n",
    "    if 'data' in data:  # SQuAD or COQA structure\n",
    "        for article in data['data']:\n",
    "            for paragraph in article.get('paragraphs', []):\n",
    "                for qa in paragraph.get('qas', []):\n",
    "                    question = qa.get('question')\n",
    "                    answer = qa['answers'][0].get('text') if qa['answers'] else None\n",
    "                    processed_data.append({'question': question, 'answer': answer})\n",
    "                    if len(processed_data) >= sample_size:\n",
    "                        return pd.DataFrame(processed_data)\n",
    "    elif 'annotations' in data:  # Other QA datasets\n",
    "        for item in data.get('annotations', []):\n",
    "            question = item['question_text']\n",
    "            answer = item['short_answers'][0].get('text') if item['short_answers'] else None\n",
    "            processed_data.append({'question': question, 'answer': answer})\n",
    "            if len(processed_data) >= sample_size:\n",
    "                return pd.DataFrame(processed_data)\n",
    "\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Function to process CSV data (e.g., Open Trivia)\n",
    "def process_csv_data(path, sample_size=1000, columns=None):\n",
    "    df = pd.read_csv(path, usecols=columns, nrows=sample_size)\n",
    "    df = df.rename(columns={'correct_answer': 'answer'})\n",
    "    return df[['question', 'answer']]\n",
    "\n",
    "# Function to process large CSV files in chunks\n",
    "def process_large_csv(path, sample_size=1000, chunksize=10000):\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(path, usecols=['question', 'correct_answer'], chunksize=chunksize):\n",
    "        chunk = chunk.rename(columns={'correct_answer': 'answer'})\n",
    "        chunk_list.append(chunk[['question', 'answer']])\n",
    "        if len(chunk_list) * chunksize >= sample_size:\n",
    "            break\n",
    "    return pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Function to process TSV data for WikiQA\n",
    "def process_tsv_data(path, sample_size=1000, columns=['Question', 'Sentence', 'Label']):\n",
    "    df = pd.read_csv(path, sep='\\t', usecols=columns, nrows=sample_size)\n",
    "    df = df.rename(columns={'Question': 'question', 'Sentence': 'answer'})\n",
    "    return df[['question', 'answer']]\n",
    "\n",
    "# Load and preprocess SQuAD dataset\n",
    "def preprocess_squad(path, sample_size=1000):\n",
    "    with open(path, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "    return process_json_data(squad_data, sample_size=sample_size)\n",
    "\n",
    "# Load and preprocess COQA dataset\n",
    "def preprocess_coqa(path, sample_size=1000):\n",
    "    with open(path, 'r') as f:\n",
    "        coqa_data = json.load(f)\n",
    "    \n",
    "    coqa_data_processed = []\n",
    "    for dialog in coqa_data['data']:\n",
    "        for q, a in zip(dialog['questions'], dialog['answers']):\n",
    "            question = q['input_text']\n",
    "            answer = a['input_text']\n",
    "            coqa_data_processed.append({'question': question, 'answer': answer})\n",
    "            if len(coqa_data_processed) >= sample_size:\n",
    "                break\n",
    "        if len(coqa_data_processed) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(coqa_data_processed)\n",
    "\n",
    "# Load and preprocess Open Trivia dataset\n",
    "def preprocess_open_trivia(path, sample_size=1000):\n",
    "    return process_large_csv(path, sample_size=sample_size)\n",
    "\n",
    "# Load and preprocess WikiQA dataset from TSV format\n",
    "def preprocess_wikiqa(path, sample_size=1000):\n",
    "    return process_tsv_data(path, sample_size=sample_size)\n",
    "\n",
    "# Paths to datasets\n",
    "squad_path = '/kaggle/input/squad-20/train-v2.0.json'  # SQuAD dataset\n",
    "coqa_path = '/kaggle/input/conversational-question-answering-dataset-coqa/coqa-train-v1.0.json'  # COQA dataset\n",
    "wikiqa_path = '/kaggle/input/wikiqa-corpus/WikiQA-train.tsv'  # WikiQA dataset\n",
    "trivia_path = '/kaggle/input/open-trivia-database-quiz-questions-all-categories/quiz_questions.csv'  # Open Trivia dataset\n",
    "\n",
    "# Process each dataset and store them in a list\n",
    "datasets = [\n",
    "    preprocess_squad(squad_path),\n",
    "    preprocess_coqa(coqa_path),\n",
    "    preprocess_wikiqa(wikiqa_path),\n",
    "    preprocess_open_trivia(trivia_path)\n",
    "]\n",
    "\n",
    "# Combine all datasets into a single DataFrame\n",
    "combined_df = pd.concat(datasets, ignore_index=True)\n",
    "print(\"\\nCombined Dataset Sample:\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Free up memory\n",
    "del datasets\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T15:35:00.421527Z",
     "iopub.status.busy": "2024-11-07T15:35:00.421136Z",
     "iopub.status.idle": "2024-11-07T16:16:32.263801Z",
     "shell.execute_reply": "2024-11-07T16:16:32.262730Z",
     "shell.execute_reply.started": "2024-11-07T15:35:00.421491Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "<class 'str'>    3260\n",
      "Name: count, dtype: int64\n",
      "answer\n",
      "<class 'str'>    3260\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a35a5a0ec049af9a7c24959a640ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a242eac1260427eab0106d108adc855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6002bc9e53e42e68fdf72c1d17c5da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf05e3086fe5400dafbdf014a35efe38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0435332c654ef3b00bf4e957a5331a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d11b75962f4fcabb2f478734c2b5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624db004dc5f47f28c6632fed59623c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927d3dcba8ab4fb9890fe34cec46a3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1101' max='1101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1101/1101 40:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.087621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.082225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.077003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [41/41 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.0770028680562973, 'eval_runtime': 29.6869, 'eval_samples_per_second': 10.981, 'eval_steps_per_second': 1.381, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have combined_df as the DataFrame containing your data\n",
    "\n",
    "combined_df['question'] = combined_df['question'].astype(str).fillna('')\n",
    "combined_df['answer'] = combined_df['answer'].astype(str).fillna('')\n",
    "\n",
    "print(combined_df['question'].apply(type).value_counts())\n",
    "print(combined_df['answer'].apply(type).value_counts())\n",
    "\n",
    "train_df, val_df = train_test_split(combined_df, test_size=0.1)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \n",
    "    inputs = bart_tokenizer(examples['question'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = bart_tokenizer(examples['answer'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    inputs['labels'] = targets['input_ids']  \n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,  \n",
    "    warmup_steps=500,  \n",
    "    weight_decay=0.01,  \n",
    "    logging_dir='./logs', \n",
    "    logging_steps=10,  \n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=3, \n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"eval_loss\"  \n",
    ")\n",
    "\n",
    "# 9. Set up the Trainer (handles training and evaluation for BART)\n",
    "trainer = Trainer(\n",
    "    model=bart_model,  # The model to train (BART)\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=train_dataset,  # The training dataset\n",
    "    eval_dataset=val_dataset,  # The validation dataset\n",
    "    tokenizer=bart_tokenizer,  # Tokenizer used for encoding the input and output\n",
    ")\n",
    "\n",
    "# 10. Train the BART model\n",
    "trainer.train()\n",
    "\n",
    "# 11. Save the trained BART model and tokenizer\n",
    "bart_model.save_pretrained('./final_bart_model1')\n",
    "bart_tokenizer.save_pretrained('./final_bart_tokenizer1')\n",
    "\n",
    "# 12. Optionally, evaluate the BART model on the validation dataset after training\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T08:53:10.833073Z",
     "iopub.status.busy": "2024-11-07T08:53:10.832124Z",
     "iopub.status.idle": "2024-11-07T08:53:14.691637Z",
     "shell.execute_reply": "2024-11-07T08:53:14.690640Z",
     "shell.execute_reply.started": "2024-11-07T08:53:10.833021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer have been saved.\n"
     ]
    }
   ],
   "source": [
    "# 11. Save the trained BART model and tokenizer\n",
    "bart_model.save_pretrained('./final_bart_model1')\n",
    "bart_tokenizer.save_pretrained('./final_bart_tokenizer1')\n",
    "\n",
    "# Confirm that the model and tokenizer have been saved\n",
    "print(\"Model and tokenizer have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T16:22:06.688960Z",
     "iopub.status.busy": "2024-11-07T16:22:06.688213Z",
     "iopub.status.idle": "2024-11-07T16:46:17.658504Z",
     "shell.execute_reply": "2024-11-07T16:46:17.657429Z",
     "shell.execute_reply.started": "2024-11-07T16:22:06.688919Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming you already have combined_df as the DataFrame containing your data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Clean the DataFrame: Convert NaN values to empty strings and ensure all columns are of type string\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Assuming you already have combined_df as the DataFrame containing your data\n",
    "\n",
    "# 1. Clean the DataFrame: Convert NaN values to empty strings and ensure all columns are of type string\n",
    "combined_df['question'] = combined_df['question'].astype(str).fillna('')\n",
    "combined_df['answer'] = combined_df['answer'].astype(str).fillna('')\n",
    "\n",
    "# 2. Check for any non-string values in 'question' and 'answer' columns\n",
    "print(combined_df['question'].apply(type).value_counts())\n",
    "print(combined_df['answer'].apply(type).value_counts())\n",
    "\n",
    "# 3. Split the dataset into train and validation sets (90% train, 10% validation)\n",
    "train_df, val_df = train_test_split(combined_df, test_size=0.1)\n",
    "\n",
    "# 4. Convert DataFrame to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# 5. Load the BART tokenizer and model\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('/kaggle/working/final_bart_tokenizer1')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('/kaggle/working/final_bart_model1')\n",
    "\n",
    "\n",
    "# 6. Tokenization function for the question-answer pairs\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize 'question' as input and 'answer' as target\n",
    "    inputs = bart_tokenizer(examples['question'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    targets = bart_tokenizer(examples['answer'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    \n",
    "    # Use the 'answer' column as labels for training (target labels)\n",
    "    inputs['labels'] = targets['input_ids']  \n",
    "    return inputs\n",
    "\n",
    "# 7. Apply the tokenization function to both the training and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 8. Set up the Trainer for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,\n",
    "    output_dir=\"./fine_tuned_bart\",  # Directory to save the model\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,  # Adjust if needed\n",
    "    save_steps=1000,    # Adjust if needed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bart_model,  # Make sure to use the loaded BART model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# 9. Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# 10. Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_bart2\")  # Ensure directory to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:15:17.087614Z",
     "iopub.status.busy": "2024-11-07T17:15:17.086506Z",
     "iopub.status.idle": "2024-11-07T17:15:20.460012Z",
     "shell.execute_reply": "2024-11-07T17:15:20.458913Z",
     "shell.execute_reply.started": "2024-11-07T17:15:17.087552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 11. Save the trained BART model and tokenizer\n",
    "# Save the fine-tuned model and tokenizer after training\n",
    "trainer.save_model(\"./fine_tuned_bart2\")  # Save the model weights\n",
    "bart_tokenizer.save_pretrained(\"./fine_tuned_bart2\")  # Save the tokenizer\n",
    "\n",
    "# Confirm the save operation\n",
    "print(\"Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:21:08.983957Z",
     "iopub.status.busy": "2024-11-07T17:21:08.983512Z",
     "iopub.status.idle": "2024-11-07T17:22:41.017604Z",
     "shell.execute_reply": "2024-11-07T17:22:41.016504Z",
     "shell.execute_reply.started": "2024-11-07T17:21:08.983919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/fine_tuned_bart2.zip'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Create zip files for the model and tokenizer\n",
    "shutil.make_archive('/kaggle/working/fine_tuned_bart2', 'zip', '/kaggle/working/fine_tuned_bart2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T17:19:29.006687Z",
     "iopub.status.busy": "2024-11-07T17:19:29.006230Z",
     "iopub.status.idle": "2024-11-07T17:19:33.171555Z",
     "shell.execute_reply": "2024-11-07T17:19:33.170480Z",
     "shell.execute_reply.started": "2024-11-07T17:19:29.006636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are glacier caves formed??\n",
      "Response: Glacier caves are often called ice caves, but this term is properly used to describe bedrock caves that contain ice.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned BART model and tokenizer\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"./fine_tuned_bart2\")  # Directory where the model is saved\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"./fine_tuned_bart2\")\n",
    "\n",
    "# Function to generate a response to a question\n",
    "def generate_response(question, max_length=256):\n",
    "    # Tokenize the input question\n",
    "    inputs = bart_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    # Generate the answer (response)\n",
    "    outputs = bart_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.2,  # Adjust if needed for more variety\n",
    "        top_p=0.9,        # Control diversity\n",
    "        do_sample=True,   # Enable sampling for diverse outputs\n",
    "    )\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "question = \"how are glacier caves formed??\"\n",
    "response = generate_response(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are glacier caves formed??\n",
      "Response: Glacier caves are often called ice caves, but this term is properly used to describe bedrock caves that contain ice.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned BART model and tokenizer\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"./fine_tuned_bart2\")  # Directory where the model is saved\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"./fine_tuned_bart2\")\n",
    "\n",
    "# Function to generate a response to a question\n",
    "def generate_response(question, max_length=256):\n",
    "    # Tokenize the input question\n",
    "    inputs = bart_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    \n",
    "    # Generate the answer (response)\n",
    "    outputs = bart_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.2,  # Adjust if needed for more variety\n",
    "        top_p=0.9,        # Control diversity\n",
    "        do_sample=True,   # Enable sampling for diverse outputs\n",
    "    )\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "question = \"how are glacier caves formed?\"\n",
    "response = generate_response(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned BART model and tokenizer\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"./fine_tuned_bart2\")  # Path to your model\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"./fine_tuned_bart2\")\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(question):\n",
    "    inputs = bart_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    outputs = bart_model.generate(inputs['input_ids'], max_length=256, num_return_sequences=1, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "    response = bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# GUI setup\n",
    "class ChatbotApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Chatbot\")\n",
    "\n",
    "        # Chat display area\n",
    "        self.chat_display = tk.Text(root, wrap=\"word\", state=\"disabled\", bg=\"#F1F3F4\", font=(\"Helvetica\", 14))\n",
    "        self.chat_display.pack(padx=10, pady=10, fill=\"both\", expand=True)\n",
    "\n",
    "        # Entry widget for user's input\n",
    "        self.entry = tk.Entry(root, font=(\"Helvetica\", 14), bg=\"#FFFFFF\")\n",
    "        self.entry.pack(padx=10, pady=10, fill=\"x\")\n",
    "        self.entry.bind(\"<Return>\", self.send_message)  # Send message on Enter key\n",
    "\n",
    "        # Button to send message\n",
    "        self.send_button = tk.Button(root, text=\"Send\", font=(\"Helvetica\", 14), command=self.send_message)\n",
    "        self.send_button.pack(pady=10)\n",
    "\n",
    "    def display_message(self, sender, message):\n",
    "        self.chat_display.config(state=\"normal\")\n",
    "        self.chat_display.insert(\"end\", f\"{sender}: {message}\\n\")\n",
    "        self.chat_display.config(state=\"disabled\")\n",
    "        self.chat_display.see(\"end\")\n",
    "\n",
    "    def send_message(self, event=None):\n",
    "        user_input = self.entry.get()\n",
    "        if user_input:\n",
    "            self.display_message(\"User\", user_input)\n",
    "            self.entry.delete(0, \"end\")\n",
    "\n",
    "            # Generate response\n",
    "            response = generate_response(user_input)\n",
    "            self.display_message(\"Chatbot\", response)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "app = ChatbotApp(root)\n",
    "root.geometry(\"500x600\")  # Optional: set the size of the window\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T12:23:47.091871Z",
     "iopub.status.busy": "2024-11-07T12:23:47.091107Z",
     "iopub.status.idle": "2024-11-07T12:40:40.504357Z",
     "shell.execute_reply": "2024-11-07T12:40:40.503424Z",
     "shell.execute_reply.started": "2024-11-07T12:23:47.091833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0560c48765040a79136ed9537cecde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ded0e839be74c36805286a79b16b1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094ac587623f4c5395bcfad03908bedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131e00655ac944458197039937d91d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ec5ccf89b74503813037dd65fd81e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ebc08438b443bc980aa17ba7bd7826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0df2e502334b7090aeb5dea2acfe1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e98378e1e04c62bfe9eb0557f6ae03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4b48ce465f4469bf3d8cbed82448b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8ba436270c40e183500e8a0295b608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112949277776351, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241107_123211-xmv48qt2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alay-patel-sit/huggingface/runs/xmv48qt2' target=\"_blank\">./results/fine_tuned_gpt2</a></strong> to <a href='https://wandb.ai/alay-patel-sit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alay-patel-sit/huggingface' target=\"_blank\">https://wandb.ai/alay-patel-sit/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alay-patel-sit/huggingface/runs/xmv48qt2' target=\"_blank\">https://wandb.ai/alay-patel-sit/huggingface/runs/xmv48qt2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1101' max='1101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1101/1101 08:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.061480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.057673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.058837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2/tokenizer_config.json',\n",
       " './fine_tuned_gpt2/special_tokens_map.json',\n",
       " './fine_tuned_gpt2/vocab.json',\n",
       " './fine_tuned_gpt2/merges.txt',\n",
       " './fine_tuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_df is the DataFrame with your data\n",
    "combined_df['question'] = combined_df['question'].astype(str).fillna('')\n",
    "combined_df['answer'] = combined_df['answer'].astype(str).fillna('')\n",
    "\n",
    "# Split into training and validation sets (90% train, 10% validation)\n",
    "train_df, val_df = train_test_split(combined_df, test_size=0.1)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure the tokenizer pads properly (GPT-2 does not use EOS token by default)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Tokenization function for input and output pairs\n",
    "def tokenize_function(examples):\n",
    "    # Prepare input-output pairs with special tokens and padding\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        examples['question'], padding=\"max_length\", truncation=True, max_length=256\n",
    "    )\n",
    "    with gpt2_tokenizer.as_target_tokenizer():\n",
    "        labels = gpt2_tokenizer(\n",
    "            examples['answer'], padding=\"max_length\", truncation=True, max_length=256\n",
    "        )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/fine_tuned_gpt2',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "trainer.save_model(\"./fine_tuned_gpt2\")\n",
    "gpt2_tokenizer.save_pretrained(\"./fine_tuned_gpt2\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 72533,
     "sourceId": 160296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 489487,
     "sourceId": 911511,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5381204,
     "sourceId": 9443693,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6030441,
     "sourceId": 9832153,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1115980,
     "sourceId": 1874653,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
